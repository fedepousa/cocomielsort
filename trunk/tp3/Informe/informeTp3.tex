\documentclass[a4paper,10pt]{article}
\usepackage{graphicx}
\usepackage{verbatim}
% \usepackage{lstlisting}
\usepackage{subfig}
\usepackage{float}
 \usepackage[spanish]{babel}   %ver bien como es
\usepackage[utf8]{inputenc}


\begin{document}

\tableofcontents

\newpage


\begin{center}
\section*{Aclaraciones Generales}
\addcontentsline{toc}{section}{Aclaraciones Generales} 

\begin{itemize}
\item La implementación de todos los algoritmos se realizó en lenguaje C++.

\item Para calcular los tiempos de ejecución de los algoritmos se utilizó la función gettimeofday(), que se encuentra en la librería $<sys/time.h>$. Dado que dicha función funciona solamente en sistemas operativos de tipo linux, se debe compilar con el flag -DTIEMPOS en este tipo de sistemas para poder hacer uso de las mismas.

\item Para la realización de los gráficos se utilizó Qtiplot
\end{itemize}

\end{center}

\newpage

\section*{Introducci\'on}
\addcontentsline{toc}{section}{Introducci\'on}

En el presente trabajo se busc\'o realizar diferentes aproximaciones a la resoluci\'on del problema MAX-SAT. El problema MAX-SAT es un problema de optimizaci\'on proveniente del problema de decisi\'on SAT. 

El problema SAT se basa es decidir si un conjunto de clausulas en forma normal conjuntiva, tiene alguna asignaci\'on de las variables que las componenen, tal que la evaluaci\'on de todas las clasulas sea verdadera con dicha asignaci\'on.

El problema SAT es un problema muy importante dentro del campo de la teoria de la complejidad, esto se debe a que SAT fue el primer problema que se identific\'o como NP-Completo. El Teorema de Cook demuestra que el algoritmo SAT pertenece a esta clase de algoritmos.

La importancia de este algoritmo no radica solamente en haber sido el primero en ser caracterizado como NP-Completo, se demostr\'o que el problema SAT puede ser reducido al problema 3-SAT, que es basicamente el mismo problema pero en el cual todas las clausulas tienen un m\'aximo de 3 literales. Adem\'as de probar la reducci\'on, se demostro que este problema tambi\'en pertenece a la clase NP-Completo (A diferencia del problema 2-SAT, para el cual se conoce un algoritmo polinomial para resolverlo). Esta reducci\'on del problema a 3-SAT es un resultado importante ya que luego para probar que otros problemas se encuentran tambien en esta clase se utilizaron reducciones a 3-SAT mostrando la equivalencia en cuanto a la complejidad de resoluci\'on.

\section*{Situaciones de la vida real que se pueden modelar utilizando MAX-SAT}
\addcontentsline{toc}{section}{Situaciones de la vida real que se pueden modelar utilizando MAX-SAT} 




\section*{Algoritmo exacto para MAX-SAT}
\addcontentsline{toc}{section}{Algoritmo exacto para MAX-SAT} 
Como su nombre lo indica, el algoritmo exacto para Max-Sat se encarga de resolver el problema exactamente, arrojando la asignacion que valida la mayor cantidad de clausulas posibles. Dado que no se conoce ning\'un algoritmo polinomial para resolver este problema, se implementaron 2 algoritmos de complejidad exponencial. Por un lado se implement\'o un algoritmo de fuerza bruta de simple implementaci\'on pero de muy baja eficiencia, en cuanto a tiempo de ejecuci\'on. Por otro lado se implemento un algoritmo exacto mediante backtracking para poder evitar visitar todas las asignaciones de las variables posibles.

\subsection*{Algoritmo de fuerza bruta}
\addcontentsline{toc}{subsection}{Algoritmo de fuerza bruta}
En este algoritmo la idea es muy simple, se generan absolutamente todas las asignaciones posibles que existen, siendo estas $2^{v}$ donde v es la cantidad de variables. Luego, por cada una de las asignaciones se verifica cuantas clausulas valida, en el momento que una asignaci\'on supera el m\'aximo de clausulas hasta el momento, se actualiza la cantidad de clausulas validadas, as\'i como cual es la asignaci\'on que gener\'o este m\'aximo.

La idea de este algoritmo es tener una resoluci\'on muy simple del problema, es claro que el tiempo de ejecuci\'on va a ser muy malo ya que se revisan todas y cada una de las asignaciones posibles, y estas crecen en orden exponencial en funci\'on de la cantidad de variables. Sin embargo, cabe destacar que el algoritmo provee una resoluci\'on exacta del problema y con baja probabilidad de errores dada la simpleza del mismo.

A continuaci\'on se presenta el pseudoc\'odigo del mismo:

\begin{verbatim}
maxSatExacto(Vector clausulas, int variables)
vector asignacion
int max := 0
inicializar asignacion todos en falso
Para i = 1 hasta 2^variables
    int sat := 0
    Para j = 1 hasta tamanio(clausulas)
       Si haceTrue(asignacion, clausulas[j])
          sat:= sat + 1
       fin si
    fin para
    si sat > max
        actualizar max
        actualizar asignacionMax
    fin si
    asignacion := siguiente(asignacion,i+1)
fin para
devolver asignacionMax, max
\end{verbatim}

Lo que muestra el pseudoc\'odigo anterior es como, por cada asignaci\'on posible, se mira cada clausula y si la funci\'on \emph{haceTrue} devuelve true, entonces se suma 1 a la cantidad de satisfechas por esa asignaci\'on. Por \'ultimo, se mira cual asignaci\'on es la que tiene m\'as clausulas satisfechas.

La funci\'on \emph{haceTrue} lo que hace es simplemente mirar toda la clausula pasada como par\'ametro y ver si algun literal esta asignado como verdadera, cuando encuentra uno deja de buscar y devuelve True. En caso contrario, si llega hasta el final de la clausula, devuelve False. 

Por otro lado, la funci\'on \emph{siguiente} se encarga de modificar para la asignaci\'on para probar con todas las posibles.

\subsection*{Algoritmo de backtracking}
\addcontentsline{toc}{subsection}{Algoritmo de backtracking}

Luego de implementar un algoritmo exacto por fuerza bruta, se busc\'o implementar un algoritmo tambi\'en exacto pero tratando de lograr un menor tiempo de ejecuci\'on. El algoritmo implementado es un algoritmo exacto basado en la t\'ecnica de backtracking para lograr mejores resultados (en cuanto a tiempo de ejecuci\'on), si bien en la siguiente secci\'on se ver\'a que la complejidad temporal es la misma para ambos algoritmos exactos, la t\'ecnica de backtracking provee de herramientas para no tener que consultar necesariamente por cada una de las asignaciones posibles, es en estas podas que este algoritmo mejora los tiempos de ejecuci\'on del anterior.

\bigskip

La idea de este algoritmo es la siguiente: se genera un arbol de asignaciones, donde cada nivel del arbol i, representa todas las asignaciones posibles desde la variable 1 hasta la variable i. Este arbol, es un arbol binario dado que se arranca de la asignacion nula y de all\'i se abren dos caminos, asignarle False a la variable 1, o asignarle True. Luego, cada rama se va bifurcando sucesivamente por cada variable nueva. Como se puede ver, en el peor caso que tengamos que recorrer todo el arbol la cantidad de asignaciones nuevamente esta dada por $2^{v}$ al igual que en el algoritmo exacto.

Una vez que se tiene el arbol de backtracking lo que se hace es comenzar a recorrer el arbol en alguna direcci\'on determinada. Cabe destacar que en el algoritmo implementado siempre se recorre primero la rama correspondiente a asignar falso a la variable y luego la otra.

La mejora del algoritmo radica en no recorrer todas las ramas posibles, esto se realiza de la siguiente manera: Al haber recorrido la primer rama del arbol llegando hasta una hoja, ya se tiene una mejor soluci\'on posible. Luego, cuando se este explorando una rama lo que se hace es fijarse si esa rama ya posee m\'as clausulas insatisfechas que la mejor soluci\'on hasta el momento. En caso afirmativo, la rama ya no sirve ya que no se podra mejorar la soluci\'on y entonces se puede descartar todo el subarbol que pende de esa rama. Entonces se realiza el backtracking para ir por otro camino posible. 

Las principales diferencias con el algoritmo de fuerza bruta son:
\begin{itemize}
\item La implementaci\'on es bastante m\'as complicada ya que como se realiza backtracking, se debe guardar los estados intermedios de toda la rama que se esta analizando para poder volver hacia atras y tomar un nuevo camino. En el algoritmo exacto, cada asignacion se contrasta con las clausulas originales por lo que solo se deben guardar una vez todas las clausulas.
\item El tiempo de ejecuci\'on deberia ser en la mayor\'ia de los casos. Si bien el peor caso no cambiar\'ia es importante destacar que las podas realizadas pueden traer grandes beneficios en cuanto al tiempo de ejecuci\'on. Si, por ejemplo, ya se tuviese una soluci\'on donde n clausulas son insatisfechas y al asignar True a la variable 1, n+1 clausulas se vuelven insatisfacibles entonces se podr\'ia podar toda una mitad del arbol.
\item El algoritmo de fuerza bruta no puede ser influenciado por alguna heur\'istica, mientras que el algoritmo con backtraking si. Lo que se quiere notar con esto, es que por m\'as que ya se tenga una solucion con n clausulas insatisfechas, el algoritmo por fuerza bruta tiene que probar todas las asignaciones posibles; mientras que el algoritmo de backtracking ya puede comenzar podando ramas que tengan m\'as de n clausulas insatisfechas.
\end{itemize}

A continuaci\'on se presenta el pseudoc\'odigo del algoritmo exacto con backtracking:

\begin{verbatim}
pseudo de backtracking
\end{verbatim}

\subsection*{Complejidad de algoritmos exactos}
\addcontentsline{toc}{subsection}{Complejidad de algoritmos exactos}

En primer lugar, se analizar\'a la complejidad del algoritmo realizado por fuerza bruta.

El mismo presenta un ciclo que se realiza $2^{v}$ veces, dado que esa es la cantidad total de asignaciones diferentes, siendo v la cantidad de variables. Una vez dentro de este ciclo, encontramos otro ciclo que itera sobre la totalidad de las clausulas, haciendo que este ciclo se realice c veces. 

Luego, dentro de este ciclo se llama a la funcion haceTrue. Esta funci\'on lo que hace es fijarse en todos los literales de la clausula si se encuentra asignado como verdadero. Dado que el hecho de fijarse es O(1), ya que es mirar un array, y dado que la clausula como mucho tiene 2*v literales (todas las variables negadas y sin negar), se puede ver que esta funci\'on es O(v).

Por \'ultimo, las otras operaciones que se realizan dentro del ciclo principal tienen menor complejidad que lo mostrado anteriormente ya que son asignaciones que toman O(1) o es la funcion siguiente que toma O(v).

Por lo mostrado anteriormente resulta que la complejidad temporal del algoritmo es O($(2^{v})$*c*v)
Como se puede ver, lo importante m\'as alla de la complejidad exacta, es que este algoritmo es exponencial en funci\'on de la cantidad de variables. Cabe destacar que si bien se podri\'an encontrar algoritmos con mejor complejidad, esta no podr\'ia ser menor que exponencial en el caso que se quieran revisar todas las asignaciones posibles.

\bigskip

Complejidad del algoritmo exacto con backtracking:

Para realizar el c\'alculo de complejidad de este algoritmo tomaremos el peor caso posible. El mismo consiste en que ninguna rama sea podada y por lo tanto se tengan que revisar todos los nodos posibles del arbol. En este caso el ciclo principal que itera sobre todas las asignaciones posibles ser\'ia O($2^{v}$), ya que este es el orden de la cantidad de nodos del arbol. 

Luego, hay que analizar cuales son las operaciones que se realizan cada vez que el algoritmo se encuentra en un nuevo nodo. En primer lugar, se llama a la funci\'on resolver, la misma tiene un funcionamiento an\'alogo al explicado en el algoritmo por fuerza bruta; se fija en cada clausula si la misma se hizo verdadera, y cuenta las insatisfacibles, por lo que esta funcion toma O(c*v) operaciones. Luego, lo que se hace es copiar todo el estado al siguiente nodo para que se pueda procesar, al realizar esto se copian varios par\'ametros. Sin embargo, el orden temporal de esta copia esta dado por el orden que toma copiar todas las clasulas, dado que los dem\'as par\'ametros que se copian tienen menor orden espacial ya que son solamente vectores. 

Se puede ver que copiar todo el estado toma O(c*v) operaciones ya que lo que se hace es copiar absolutamente todas las clausulas, donde cada una pueda tener hasta 2*v literales por lo justificado anteriormente.

Por \'ultimo, resumiendo lo anteriormente explicado se puede ver que cuando se revisa un nodo se toma O(c*v) operaciones, y en el peor caso hay que revisar O($2^{v}$) nodos; por lo que la complejidad total de este algoritmo es O($(2^{v})$*c*v).

Como se puede ver, la complejidad del algoritmo de backtracking no es mejor que el algoritmo exacto por fuerza bruta, sigue siendo exponencial. De hecho, se puede notar que en el peor caso posible, el algoritmo de backtracking es m\'as lento que el algoritmo por fuerza bruta dado el overhead que produce copiar todo el estado al siguiente nodo del arbol para que se pueda procesar.

Sin embargo, en la mayor\'ia de los casos, el algoritmo con backtracking presenta resultados temporalmente mejores que el algoritmo de fuerza bruta ya que se podan varias ramas haciendo que no se tenga que visitar todas las asignaciones posibles.

\medskip

Cabe destacar que la ventaja temporal que gana el algoritmo de backtracking, genera una mayor complejidad espacial ya que en todo momento se debe tener todos los estados de los nodos de la rama que se esta visitando para poder caminar hacia atras. Luego la complejidad espacial del algoritmo de fuerza bruta es O(c*v) ya que se guardan una vez todas las clausulas, mientras que la complejidad espacial del algoritmo de backtracking es O(c*v*v) ya que se guardan todas las clausulas una vez por cada nodo de la rama que se esta analizando, que a lo sumo tiene v nodos.




\section*{Heur\'istica constructiva para MAX-SAT}
\addcontentsline{toc}{section}{Heur\'istica constructiva para MAX-SAT} 
En esta secci\'on se explicar\'a el uso de una heur\'istica constructiva para resolver el problema.

La motivaci\'on principal de utilizar diferentes heur\'isticas para resolver el problema de Max-Sat es que, como se vio en la secci\'on anterior, los algoritmos exactos conocidos son de orden exponencial por lo que solo se pueden correr con instancias relativamente peque\~{n}as.

La idea entonces es realizar un algoritmo que no sea exacto, sino que arroje una soluci\'on aproximada, pero en un tiempo polinomial para poder correr instancias m\'as grandes.

En esta primer aproximaci\'on, se realiz\'o una heur\'istica constructiva, esto quiere decir que se va construyendo una soluci\'on mediante alg\'un criterio que se supone (y luego se ver\'a que se puede demostrar) que es adecuado para obtener un buen resultado.

En este caso, la heur\'istica constructiva seguir\'a una estrategia golosa para ir armando la soluci\'on. Lo que hace esta heur\'istica es revisar todas las clausulas buscando cual es el literal que m\'as se repite. Una vez encontrado dicho literal lo que se hace es asignar True a este literal y actualizar las clausulas en base a esta asignaci\'on.

Actualizar las clausulas consiste en dos tareas, en primer lugar se borran todas las clausulas que contenian este literal ya que ya fueron satisfechas mediante la asignaci\'on y se actualiza el contador de clausulas satisfechas. Por otro lado, en las clausulas restantes, se borra el literal negado ya que este no sirve para futuras elecciones porque al asignar True al literal elegido, el literal negado tendra necesariamente valor falso.

Luego de realizar esta actualizaci\'on, se continua iterativamente con esta estrategia golosa hasta encontrar una asignaci\'on completa de las variables.

A continuaci\'on se muestra el pseudoc\'odigo de la heur\'istica constructiva:

\begin{verbatim}
Constructiva(Vector clausulas, int V)
Max := 0
Comenzar con asignacion vacia
mientras asignacion no este completa
      Tomar literal l que mas se repita
      Asignar True a l
      Para i de 1 hasta tamanio(clausulas)
         Si esta(l, clausula[i])
             borrar clausula[i]
             Max := Max + 1
         fin si
      fin para
      Para i de 1 hasta tamanio(clausulas)
         Si esta(-l, clausula[i])
             borrar(-l, clausula[i])
         fin si
      fin para
fin mientras
devolver asignacion y Max
\end{verbatim}

\subsection*{Aproximaci\'on del resultado}
\addcontentsline{toc}{subsection}{Aproximaci\'on del resultado}

Es importante destacar que a priori esta heur\'istica podr\'ia arrojar resultados que no se acercasen al verdadero valor del problema. La heur\'istica parece razonable ya que siempre se elige el literal que m\'as clausulas satisface, y luego se va iterando hasta que no queden m\'as variables para asignar. Es importante destacar que si bien la heur\'istica parece razonable se puede encontrar rapidamente casos en los cuales esta aproximaci\'on golosa no nos devuelva el resultado exacto. Dada estas observaciones, se busc\'o encontrar algun tipo de cota para el resultado entregado por la heur\'istica, en funci\'on del verdadero resultado.

Dada esta heur\'istica golosa, se puede ver que en realidad se trata de un algoritmo aproximado. Si se denota $c_{max}$ como la cantidad m\'axima de clausulas satisfacibles, $c_h$ al resultado entregado por la heur\'istica y m al m\'inimo n\'umero de literales contenido en cualquiera de las clausulas, se cumple que\footnote{Cita Bibliogr\'afica 1}:


\begin{equation}
c_h \geq \frac{m}{m+1}*c_{max}
\end{equation}

Dada la ecuaci\'on anterior, se tiene una cota para el resultado que depende del m\'inimo n\'umero de literales en cualquier clausula. Cabe destacar que en la implementaci\'on de esta heur\'istica, y en los casos de prueba utilizados, no se hace ninguna menci\'on particular a la cantidad m\'inima de literales de las clausulas. En este caso, podemos acotar por el peor caso que ser\'ia tener clausulas de un solo literal.

En dicho caso, la heur\'istica utilizada ser\'ia un algoritmo $\frac{1}{2}$-aproximado, es decir que el resultado arrojado jam\'as se encontrara por debajo de la mitad del valor real del problema. En el caso del que el m\'inimo n\'umero de clausulas suba (lo que suele pasar en la pr\'actica), la cota tambi\'en se ajustar\'a dando una mejor aproximaci\'on.

\subsection*{Complejidad de la heur\'istica constructiva}
\addcontentsline{toc}{subsection}{Complejidad de la heur\'istica constructiva}

No quiero mentir mucho asi que pregunto y despues escribo



\section*{Heur\'istica de b\'usqueda local para MAX-SAT}
\addcontentsline{toc}{section}{Heur\'istica de b\'usqueda local para MAX-SAT} 

La segunda he\'uristica implementada fue la de busqueda local. Como su nombre lo indica, lo que hace esta heur\'istica es pararse sobre una soluci\'on y buscar localmente si se puede mover a otra soluci\'on \emph{cercana} con m\'as clausulas satisfechas. Dada la explicaci\'on anterior, se desprende que entonces se debe definir que significa que dos soluciones cercanas. Teniendo una asignaci\'on posible, se define una vecindad de asignaciones mediante alg\'un criterio que ser\'an las asignaciones \emph{cercanas} mencionadas anteriormente.

La idea principal de la heur\'istica es empezar con alguna asignaci\'on posible y luego buscar en la vecindad de esta soluci\'on si existe alguna asignacion que satisfaga mayor cantidad de variables, si existe esta nueva asignaci\'on, entonces se cambia la asignaci\'on m\'axima actual por esta nueva y se vuelve a realizar una busqueda local, ahora con la vecindad de la nueva asignaci\'on.

Esta heur\'istica finaliza cuando tenemos una asignaci\'on que es la mejor de todas entre ella y las asignaciones de su vecindad. Es decir, cuando se encuentra un m\'inimo local, la heur\'istica para y devuelve la mejor asignaci\'on encontrada hasta el momento.

Es importante destacar que la heur\'istica de busqueda local necesita de una asignaci\'on inicial para comenzar a definir su vecindad y luego realizar la busqueda. Como una primera aproximaci\'on se podr\'ia inicializar la heur\'istica con cualquier asignaci\'on, por ejemplo todas las variables en falso, el problema es que para caso de prueba diferente el espacio de asignaciones tiene una forma diferente. Esto puede llevar a que si inicializamos en cualquier asignaci\'on, se encuentre rapidamente un m\'aximo local que poco tenga que ver con la soluci\'on real del problema. 

La soluci\'on que se utiliz\'o para el problema explicado anteriormente es que la heur\'istica de busqueda local sea inicializada con el resultado proviniente de aplicar la heur\'istica constructiva.

Es importante notar que la heur\'istica de busqueda local no tiene permitido moverse a una asignaci\'on que satisfaga una menor cantidad de clausulas, por lo que se puede ver que el resultado arrojado por la busqueda local es mejor o igual que el resultado de la heur\'istica constructiva en el caso de que se inicialice el procedimiento con el resultado de la heur\'istica constructiva.

De esta manera, reemplazando en la ecuaci\'on (1) (denotado $c_{local}$ al resultado de la busqueda local):

\begin{equation}
c_{local} \geq c_{constr} \geq \frac{m}{m+1}*c_{max}
\end{equation}

Luego, dada la ecuaci\'on anterior y con una explicaci\'on an\'aloga a la propuesta en la heur\'istica constructiva, se puede ver que esta heur\'istica (inicializada de esta manera) tambi\'en se trata de un algoritmo $\frac{1}{2}$-aproximado y, de la misma manera, a medida que el m va subiendo, la cota va mejorando.


\subsection*{Detalles de implementaci\'on}
\addcontentsline{toc}{subsection}{Detalles de implementaci\'on}

En primer lugar, es importante destacar como se definio la vecindad de una asignaci\'on, para tener bien definido a donde se va a realizar la busqueda local dada una asignaci\'on. Se determin\'o que la vecindad de una asignaci\'on esta dada por todas las asignaciones que solo distan en una variable. Es decir, las asignaciones que tienen todas las variables asignadas igual excepto por una.

Luego, se decidio crear una estructura que dada cada variable, se tiene listado en que clausulas aparece y si el literal que aparece es la variable o su negaci\'on. Sumado a esto, se tiene listado por cada clausula que literales tienen. Por otro lado, se tiene un vector de valores de verdad que indican los estados de las variables y un vector de enteros que indica por cada clausula cuantos literales hacen verdadera la clausula.

Dada esta estructura, se procede hacer la busqueda local hasta caer en una soluci\'on inmejorable, respecto de la vecindad definida anteriormente. Es decir, se toma la asignaci\'on actual y se va modificando de a una variable y se actualizan las estructuras adecuadamente para ver si esta nueva configuraci\'on es mejor que la actual. 

El pseudoc\'odigo de la heur\'istica de busqueda local es:

\begin{verbatim}
BusquedaLocal(Vector clausulas, int V)
Max := HeuristicaConstructiva(clausulas, v)
asignacionMax := HeuristicaConstructiva(clausulas, v)
mientras cambie la asignacion
      asignacionMaxTemp := asignacionMax
      para toda asignacion_i en la vecidad de asignacionMax
         si asignacion_i satisface mas que asignacionMax
            asignacionMaxTemp := asignacion_i
         fin si
      fin para
      asignacionMax := asignacionMaxTemp
fin mientras
devolver asignacion y Max
\end{verbatim}

\subsection*{Complejidad de la heur\'istica de busqueda local}
\addcontentsline{toc}{subsection}{Complejidad de la heur\'istica de busqueda local}

ver un poco mas para no mentir



\section*{Metaheur\'istica de b\'usqueda tab\'u para MAX-SAT}
\addcontentsline{toc}{section}{Metaheur\'istica de b\'usqueda tab\'u para MAX-SAT} 

La meteheur\'istica de busqueda tabu surge para mejorar la heur\'istica de busqueda local, ya que esta puede caer en m\'aximos locales que esten muy lejos del m\'aximo absoluto del problema.

Es por esto que la busqueda tab\'u resulta de modificar la b\'usqueda local con la motivaci\'on de poder salir de asignaciones que resultan \'optimas localmente; es decir, asignaciones que satisfacen una cantidad de clausulas mayor o igual a las que satisface cualquier otra asignaci\'on en su vecindad.

La metaheur\'istica de b\'usqueda tab\'u consiste basicamente en modificar el vecindario de cada asignaci\'on. Esto se realiza excluyendo asignaciones que fueron visitadas recientemente y relajando el criterio de parada de la b\'usqueda local, permitiendo que se continue la b\'usqueda a\'un cuando la mejor asignaci\'on del vecindario no aumente la cantidad de clausulas satisfechas. 

Es importante destacar que la exclusi\'on de las asignaciones visitadas m\'as recientemente no es un detalle menor. Si no se hiciese esto se entrar\'ia en un cilco entre el \'optimo local y la mejor asignaci\'on de su vecindad, sin poder alejarnos de estas asignaciones, para buscar mejores soluciones.

Luego de lo explicado anteriormente, se denota que para poder separar las asignaciones visitadas recientemente se debe implementar alg\'un tipo de memoria. Una aproximaci\'on inicial ser\'ia guardar una lista de longitud acotada con las \'ultimas asignaciones visitadas y excluir dichas asignaciones del vecindario que se considere. Se denominar\'a lista Tab\'u a dicha lista.

En esta primera implementaci\'on presentada, ya se pueden encontrar varias sutilezas que resultan factores importantes para el desarrollo de la metaheur\'istica. Por ejemplo, la longitud de la lista Tab\'u es acotada y la cota para la misma puede ser fija, pero tambi\'en podr\'ia estar expresada en funci\'on de alg\'un par\'ametro o incluso podr\'ia variar durante la ejecuci\'on. Algo similar ocurre con el criterio de parada; es com\'un que est\'e expresado como una combinaci\'on de restricciones en la cantidad m\'axima de iteraciones y la cantidad de iteraciones desde la \'ultima asignaci\'on que mejor\'o la cantidad conocida de clausulas satisfechas.

Si bien la primera aproximaci\'on indica que la lista Tab\'u ser\'ia una lista de asignaciones, es importante destacar que esta implementaci\'on resulta ineficiente, debido a que se debe recorrer toda la lista para saber si una asignaci\'on est\'a o no. Esto resulta ineficiente adem\'as porque la comparacion entre asignaciones es lenta, ya que es necesario compara variable por variable para ver si se realmente coinciden.


La implementación de la lista Tabú como una lista de asignaciones resulta ineficiente, debido a que se debe recorrer toda la lista para saber si una asignación está o no y porque la comparación entre asignaciones resulta lenta, ya que es necesario comparar variable por variable. 
Una alternativa que se encarga del último problema consiste en representar el vecindario de una asignación en términos de movimientos que se pueden realizar desde la misma para llegar hasta otra asignación. Si los movimientos que consideramos son reversibles, entonces podemos guardar en la lista Tabú los movimientos opuestos a los que realizamos, evitando de esta manera entrar en un ciclo. Vamos a llamar movimientos Tabú a aquellos que se encuentran en la lista Tabú.
Podría ocurrir que un movimiento a una asignación que todavía no visitamos sea Tabú, y ésta asignación puede incluso satisfacer más cláusulas que cualquier otra visitada con anterioridad. En casos como éste sería deseable ignorar la condición de Tabú del movimiento. De esta manera surge la función de aspiración. En este problema decidimos ignorar la condición de Tabú de un determinado movimiento aplicado a una asignación si la asignación resultante satisface más cláusulas que la mejor asignación visitada hasta el momento.

Implementación:
Habiendo definido conceptualmente los componentes de la búsqueda Tabú se pueden mencionar detalles de implementación. 
La vecindad que definimos para cada asignación está compuesta por las asignaciones resultantes de cambiar el valor de exactamente una variable de la asignación original. Cada movimiento es el cambio de una variable, por lo tanto manteniendo un arreglo de enteros del tamaño de la cantidad de variables podemos llevar un registro de cuál fue la última iteración en la que se aplicó el movimiento i. De ésta manera podemos saber si un movimiento es Tabú con sólo ver si pasaron suficientes iteraciones desde la última iteración en la que fue usado, lo que resulta más eficiente que recorrer una lista con los movimientos. 












\section*{Bibliograf\'ia}
\addcontentsline{toc}{section}{Bibliografia}

\begin{itemize}
\item Algorithms for the Maximum Satiasfiability Problem 
\end{itemize}

\end{document}
